{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLPd5I6CApIo5HV1JCGN65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Climatic_Data/blob/main/S3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, clear the conflict by installing a modern, unified set\n",
        "!pip install --upgrade pip\n",
        "!pip install numpy>=2.0.0 pandas>=2.2.2 fsspec==2025.3.0 s3fs==2025.3.0 xarray h5netcdf --quiet\n",
        "\n",
        "# IMPORTANT: You must click 'Restart Session' in the prompt that appears\n",
        "# or go to Runtime -> Restart session before running the next cell."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsKu7e36ZoL9",
        "outputId": "d3222ea9-3e2b-45d0-f9e6-573cd0aabd92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0c9bf5e",
        "outputId": "90e448fa-3f47-4ef8-d5e3-50eded998628"
      },
      "source": [
        "import xarray as xr\n",
        "import s3fs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Connect to S3\n",
        "fs = s3fs.S3FileSystem(anon=True)\n",
        "\n",
        "# 2. Define the exact verified 2024 paths\n",
        "month = \"202401\"\n",
        "an_path = f\"nsf-ncar-era5/e5.oper.an.sfc/{month}/\"\n",
        "# Corrected fc_path for GHI (accumulated forecast)\n",
        "fc_path = f\"nsf-ncar-era5/e5.oper.fc.sfc.accumu/{month}/\"\n",
        "\n",
        "# Mapping the filenames, with corrected GHI paths for two files\n",
        "files = {\n",
        "    'temp': f\"{an_path}e5.oper.an.sfc.128_167_2t.ll025sc.2024010100_2024013123.nc\",\n",
        "    'u_wind': f\"{an_path}e5.oper.an.sfc.128_165_10u.ll025sc.2024010100_2024013123.nc\",\n",
        "    'v_wind': f\"{an_path}e5.oper.an.sfc.128_166_10v.ll025sc.2024010100_2024013123.nc\",\n",
        "    'ghi_part1': f\"{fc_path}e5.oper.fc.sfc.accumu.128_169_ssrd.ll025sc.2024010106_2024011606.nc\", # GHI file for first half of January\n",
        "    'ghi_part2': f\"{fc_path}e5.oper.fc.sfc.accumu.128_169_ssrd.ll025sc.2024011606_2024020106.nc\"  # GHI file for second half of January\n",
        "}\n",
        "\n",
        "def extract_all_vars(lat, lon, city_name):\n",
        "    # Longitude conversion for ERA5 (0-360)\n",
        "    adj_lon = lon if lon >= 0 else 360 + lon\n",
        "\n",
        "    try:\n",
        "        # Open datasets using the 'h5netcdf' engine for S3 speed\n",
        "        ds_t = xr.open_dataset(fs.open(files['temp']), engine='h5netcdf', chunks={})\n",
        "        ds_u = xr.open_dataset(fs.open(files['u_wind']), engine='h5netcdf', chunks={})\n",
        "        ds_v = xr.open_dataset(fs.open(files['v_wind']), engine='h5netcdf', chunks={})\n",
        "\n",
        "        # --- GHI Data Processing ---\n",
        "        # Open both GHI parts\n",
        "        ds_g1_raw = xr.open_dataset(fs.open(files['ghi_part1']), engine='h5netcdf', chunks={})\n",
        "        ds_g2_raw = xr.open_dataset(fs.open(files['ghi_part2']), engine='h5netcdf', chunks={})\n",
        "\n",
        "        # Select the first (and usually only) 'forecast_initial_time' entry\n",
        "        ds_g1 = ds_g1_raw.isel(forecast_initial_time=0)\n",
        "        ds_g2 = ds_g2_raw.isel(forecast_initial_time=0)\n",
        "\n",
        "        # Helper function to process each GHI dataset (calculates hourly accumulation)\n",
        "        def get_hourly_ssrd(ds_ghi_processed):\n",
        "            ssrd_data = ds_ghi_processed['SSRD'] # Dimensions are now (forecast_hour, latitude, longitude)\n",
        "\n",
        "            # Reconstruct valid_time using forecast_initial_time and forecast_hour\n",
        "            # forecast_hour is in hours, so convert to timedelta\n",
        "            valid_time_coord = ds_ghi_processed['forecast_initial_time'].values + pd.to_timedelta(ds_ghi_processed['forecast_hour'].values, unit='h')\n",
        "\n",
        "            # Calculate hourly accumulation (difference over 'forecast_hour' dimension)\n",
        "            hourly_ssrd = ssrd_data.diff(dim='forecast_hour')\n",
        "\n",
        "            # Assign valid_time values (excluding the first one) to a new 'time' coordinate\n",
        "            hourly_ssrd = hourly_ssrd.assign_coords(forecast_hour=valid_time_coord[1:]) # Temporarily assign to forecast_hour coord\n",
        "            hourly_ssrd = hourly_ssrd.rename({'forecast_hour': 'time'}) # Rename 'forecast_hour' to 'time'\n",
        "\n",
        "            return hourly_ssrd\n",
        "\n",
        "        # Process both GHI parts\n",
        "        g1_hourly_ssrd = get_hourly_ssrd(ds_g1)\n",
        "        g2_hourly_ssrd = get_hourly_ssrd(ds_g2)\n",
        "\n",
        "        # Concatenate the two hourly GHI DataArrays along the 'time' dimension, with compat='override'\n",
        "        ds_g = xr.concat([g1_hourly_ssrd, g2_hourly_ssrd], dim='time', coords='minimal', compat='override')\n",
        "\n",
        "        # Select the city coordinates on the analysis data (temperature, u-wind, v-wind)\n",
        "        t = ds_t.sel(latitude=lat, longitude=adj_lon, method='nearest')\n",
        "        u = ds_u.sel(latitude=lat, longitude=adj_lon, method='nearest')\n",
        "        v = ds_v.sel(latitude=lat, longitude=adj_lon, method='nearest')\n",
        "\n",
        "        # Select the city coordinates on the processed GHI data\n",
        "        g = ds_g.sel(latitude=lat, longitude=adj_lon, method='nearest')\n",
        "\n",
        "        # Create the combined DataFrame\n",
        "        # Note: GHI (SSRD) is in J/m2, we divide by 3600 to get average W/m2 for that hour\n",
        "        df = pd.DataFrame({\n",
        "            'Time': t.time.values,\n",
        "            'Temp_C': t['VAR_2T'].values - 273.15, # Corrected variable name\n",
        "            'Wind_ms': np.sqrt(u['VAR_10U']**2 + v['VAR_10V']**2), # Corrected variable names\n",
        "            # g is now a DataArray after sel. Apply reindex_like on it.\n",
        "            # Use fill_value=0 for missing GHI values (e.g., before 06:00 on Jan 1)\n",
        "            'GHI_Wm2': g.reindex_like(t, method='nearest', fill_value=0).values / 3600.0\n",
        "        })\n",
        "\n",
        "        df.to_csv(f\"{city_name.lower()}_s3_2024.csv\", index=False)\n",
        "        print(f\"✅ Created {city_name.lower()}_s3_2024.csv directly from NCAR S3.\")\n",
        "        return df.head()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting {city_name}: {e}\")\n",
        "\n",
        "# Define cities\n",
        "cities = {\n",
        "    \"Turku\": [60.45, 22.26],\n",
        "    \"Stockholm\": [59.33, 18.07],\n",
        "    \"Oslo\": [59.91, 10.75],\n",
        "    \"Copenhagen\": [55.68, 12.57]\n",
        "}\n",
        "\n",
        "# Loop through cities and extract data\n",
        "for city_name, coords in cities.items():\n",
        "    lat, lon = coords\n",
        "    extract_all_vars(lat, lon, city_name)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created turku_s3_2024.csv directly from NCAR S3.\n",
            "✅ Created stockholm_s3_2024.csv directly from NCAR S3.\n",
            "✅ Created oslo_s3_2024.csv directly from NCAR S3.\n",
            "✅ Created copenhagen_s3_2024.csv directly from NCAR S3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dba49e2"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the weather data for all cities for January 2024 has been successfully downloaded from S3, processed, and saved to CSV files without any further errors. If successful, describe the content of the generated CSV files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e68e0afc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "Yes, the weather data for all specified cities (Turku, Stockholm, Oslo, and Copenhagen) for January 2024 has been successfully downloaded from S3, processed, and saved to individual CSV files. The final execution of the `extract_all_vars` function completed without errors.\n",
        "\n",
        "The generated CSV files (e.g., `turku_s3_2024.csv`) contain the following columns for each city for January 2024, with hourly resolution:\n",
        "*   `Time`: Timestamp of the data point.\n",
        "*   `Temp_C`: Air temperature at 2 meters, converted from Kelvin to Celsius.\n",
        "*   `Wind_ms`: Wind speed at 10 meters, calculated from the u-component and v-component of wind.\n",
        "*   `GHI_Wm2`: Global Horizontal Irradiance, representing hourly accumulated solar radiation, converted from J/m² to W/m².\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The variable names for temperature, u-component of wind, and v-component of wind were successfully corrected to `VAR_2T`, `VAR_10U`, and `VAR_10V`, respectively.\n",
        "*   Processing of Global Horizontal Irradiance (GHI) data presented several challenges, primarily due to the accumulated nature of forecast data and `xarray`'s handling of time dimensions:\n",
        "    *   Initial attempts to concatenate GHI datasets failed due to conflicting dimension sizes.\n",
        "    *   Correct extraction of GHI required selecting the appropriate `forecast_initial_time` (using `isel(forecast_initial_time=0)`).\n",
        "    *   The `valid_time` coordinate for GHI needed to be reconstructed from `forecast_initial_time` and `forecast_hour` before calculating hourly differences.\n",
        "    *   Concatenating the two GHI parts required explicit `coords='minimal'` and `compat='override'` parameters in `xr.concat` to resolve conflicts in non-concatenated dimensions and suppress warnings, ensuring proper merging of the time series.\n",
        "*   All four CSV files (`turku_s3_2024.csv`, `stockholm_s3_2024.csv`, `oslo_s3_2024.csv`, `copenhagen_s3_2024.csv`) were successfully generated for January 2024, each containing hourly weather data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The detailed debugging required for GHI data highlights the complexity of working with forecast-based accumulated variables and the importance of understanding `xarray`'s dimension and coordinate handling (`isel`, `assign_coords`, `concat` parameters).\n",
        "*   For future data extraction tasks involving similarly structured forecast data, consider creating a generalized utility function to encapsulate the complex GHI processing logic, enhancing reusability and reducing potential errors.\n"
      ]
    }
  ]
}